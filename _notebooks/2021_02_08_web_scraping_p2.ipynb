{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021-02-08-web_scraping_p2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# pt-BR | Web scraping - Beautiful Soup\n",
        "> \"Beautiful Soup & Python.\"\n",
        "- toc: false\n",
        "- branch: master\n",
        "- badges: true\n",
        "- comments: false\n",
        "- categories: [fastpages, jupyter]\n",
        "- hide: false\n",
        "- author: leticiaplang"
      ],
      "metadata": {
        "id": "-UaM6oghpLam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seguindo com o **web scraping**, teremos nesse notebook a coleta de dados utilizando o **Beautiful Soup**. Diferente o Selenium, n√£o utilizaremos janela de web para realizar o processo, faremos isso √†s cegas por meio de comendos."
      ],
      "metadata": {
        "id": "0xb-HpEIYK_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Web scraping com Beautiful Soup e Python"
      ],
      "metadata": {
        "id": "PPDtKbcIZGsf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para inciarmos o processo manteremos a realiza√ß√£o de todas as instala√ß√µes ou importa√ß√µes das bibliotecas ou pacotes que iremos utilizar. \n",
        "* **bs4** √© a biblioteca que cont√©m o beautiful soup. A caracter√≠stica dessa fun√ß√£o √© varrer todos os dados da p√°gina web (HTML), identificando grupos que contenham as informa√ß√µes que queremos. \n",
        "* **Pandas** √© essencial para transformarmos os dados em estrutura de data frame, que possui mesma estrutura de uma tabela (clolunas e linhas).\n"
      ],
      "metadata": {
        "id": "s3hBM5URYLCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa√ß√µes\n",
        "import time  #se necess√°rio utilizar um tempo de espera para rodar as pr√≥ximas c√©lulas\n",
        "from bs4 import BeautifulSoup #biblioteca que cont√©m m√≥dulo que utilizaremos\n",
        "import pandas as pd #dataframe e explora√ß√£o dos dados"
      ],
      "metadata": {
        "id": "TFcx_NlmY5oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesse artigo j√° abordarei o web scraping automatizado para rodar at√© n p√°ginas definidas por mim. Poder√≠amos ter determinado esse valor automaticamente, retirando informa√ß√µes do pr√≥prio site, mas por hora abordarei deste modo para n√£o dificultar a compreens√£o.\n",
        "\n",
        "Achou muito confuso? N√£o se preocupe que irei abordar etapa por etapa desse c√≥digo üòÉ "
      ],
      "metadata": {
        "id": "acXYUOJbuuZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_orig = pd.DataFrame(columns=['product', \n",
        "                               'area', \n",
        "                               'room', \n",
        "                               'bath', \n",
        "                               'garage', \n",
        "                               'adress', \n",
        "                               'price'])\n",
        "page = 1\n",
        "\n",
        "while page <=3:\n",
        "    #¬†Viva Real homepage\n",
        "    url = f'https://www.imoveis.com.br/aluguel/sao-paulo/saopaulo/?pagina={page}'\n",
        "    time.sleep(3)    \n",
        "    #¬†Usaremos o request para realizar a requisi√ß√£o\n",
        "    vr_response = requests.get(url)\n",
        "    time.sleep(2)\n",
        "    #¬†Convers√£o da resposta para HTML Usando Beautiful Soup\n",
        "    vr_soup = BeautifulSoup(vr_response.text, 'html.parser')\n",
        "    time.sleep(2)\n",
        "    prod_lists = [product.text.lower() for product in vr_soup.find_all('span', {'class':'property-card__title'})]\n",
        "    area_lists = [area.text.lower() for area in vr_soup.find_all('li', {'class':'property-card__detail-area'})]\n",
        "    room_lists = [room.text.lower() for room in vr_soup.find_all('li', {'class':'property-card__detail-room'})]\n",
        "    bath_lists = [bath.text.lower() for bath in vr_soup.find_all('li', {'class': 'property-card__detail-bathroom'})]\n",
        "    garage_lists = [garage.text.lower() for garage in vr_soup.find_all('li', {'class':'property-card__detail-garage'})]\n",
        "    adress_lists = [adress.text.lower() for adress in vr_soup.find_all('span', {'class':'property-card__address'})]\n",
        "    price_lists = [price.text.lower() for price in vr_soup.find_all('div', {'class':'property-card__price'})]\n",
        "    page += 1\n",
        "    df_orig = df_orig.append(pd.DataFrame({'product':prod_lists, \n",
        "                                           'area':area_lists, \n",
        "                                           'room':room_lists, \n",
        "                                           'bath':bath_lists, \n",
        "                                           'garage':garage_lists, \n",
        "                                           'adress':adress_lists, \n",
        "                                           'price':price_lists}))"
      ],
      "metadata": {
        "id": "PVvl8XhJuu3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quando utilizamos o **while** precisamos ter um limite que se torne falso para que voc√™ n√£o entre em um looping infinito dos dados. Nesse caso, defino o n√∫mero de p√°ginas que vou querer analisar, iniciando pela p√°gina 1. Perceba que a p√°gina est√° fora do loop while, sen√£o ela iria sempre reiniciar na 1 a cada vez que um novo ciclo se iniciasse, caindo no tal loop infinito.\n",
        "\n",
        "Dentro do while defino a p√°gina que √© de meu interesse realizar o web scraping. Nessa situa√ß√£o inserimos a **p√°gina que est√° com os dados que queremos coletar**, ou seja, se eu quiser buscar im√≥veis de S√£o Paulo, preciso previamente ir no site, definir os crit√©rios de busca para ent√£o saber qual √© a URL que irei utilizar para a coleta com b4s.\n",
        "\n",
        "Ap√≥s isso, devemos analizar a URL do site. Clique para ir a pr√≥xima p√°gina e identifique **como a URL se estrutura**. Essa √© a que iremos inserir no loop para que possamos estar fazendo o scrapping de todas as p√°ginas do site.\n",
        "\n",
        "No local da URL que possui o n√∫mero dois (segunda p√°gina) iremos trocar por chaves que componham a vari√°vel p√°gina que definimos previamente. \n",
        "\n",
        "Ap√≥s definida a URL, precisamos solicitar acesso para pegarmos os dados. Salvamos em uma vari√°vel, pois a resposta conter√° informa√ß√µes do site que estamos buscando. Por tal motivo √© que extra√≠mos o texto dessa resposta para utilizarmos o \"buscador de informa√ß√µes\" do b4s chamado html.parser nesse caso.\n"
      ],
      "metadata": {
        "id": "XBjIMSXVyS_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page = 1\n",
        "\n",
        "while page <=3:\n",
        "    #¬†Im√≥veis homepage\n",
        "    url = f'https://www.imoveis.com.br/aluguel/sao-paulo/saopaulo/?pagina={page}'\n",
        "    time.sleep(3)    \n",
        "    #¬†Usaremos o request para realizar a requisi√ß√£o\n",
        "    vr_response = requests.get(url)\n",
        "    time.sleep(2)\n",
        "     #¬†Convers√£o da resposta para HTML Usando Beautiful Soup\n",
        "    vr_soup = BeautifulSoup(vr_response.text, 'html.parser')\n",
        "    time.sleep(2)"
      ],
      "metadata": {
        "id": "59xNZzTvySXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tendo a URL definida com os dados que queremos, precisamos ent√£o descobrir quais s√£o os elementos que buscamos na p√°gina. Voc√™ lembra como se faz? ü§î\n",
        "\n",
        "No web scraping parte 1 abordei como podemos realizar a inspe√ß√£o do elemento da p√°gina web e encontrar os elementos que caracterizem o dado que buscamos. Maaas, spoiler alert, n√£o h√° f√≥rmula perfeita, voc√™ precisar√° colocar a m√£o na massa e descobrir por tentativa e erro.\n",
        "\n",
        "Iremos aqui trabalhar com **list comprehensions** üòé e a l√≥gica da constru√ß√£o √© a seguinte:\n",
        "* Definimos qual o output que queremos\n",
        "* Definimos um for loop para que possamos passar dado a dado e transformar adequadamente cada um deles\n",
        "* Inserimos dentro de dois colchetes que definem que o outpur ser√° uma lista de valores.\n",
        "\n",
        "Aqui defino como output cada nome de produto ser√° texto min√∫sculo. \n",
        "\n",
        "O for loop ir√° procurar cada valor dentro dos valores que o \"buscador de valores\" do b4s encontrou no site que fecham os os identificadores que defini. \n",
        "  \n",
        "*vr_soup* √© a vari√°vel que cont√©m os dados HTML do site e o buscador\n",
        "\n",
        "*find_all* define que queremos que o \"buscador\" traga todos os itens que possuem a caracter√≠stica de definimos\n",
        "\n",
        "* a caracter√≠stica √© definida por: (grande grupo(elemento que identifica determinada informa√ß√£o dentro do grande grupo))\n",
        "* Perceba que difrentemente do selenium, devemos informar que √© uma classe para depois informar a designa√ß√£o do delemento."
      ],
      "metadata": {
        "id": "L6e838WT0Ope"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # Definindo a constru√ß√£o das listas para cada tipo de informa√ß√£o\n",
        "    prod_lists = [product.text.lower() for product in vr_soup.find_all('span', {'class':'property-card__title'})]\n",
        "    area_lists = [area.text.lower() for area in vr_soup.find_all('li', {'class':'property-card__detail-area'})]\n",
        "    room_lists = [room.text.lower() for room in vr_soup.find_all('li', {'class':'property-card__detail-room'})]\n",
        "    bath_lists = [bath.text.lower() for bath in vr_soup.find_all('li', {'class': 'property-card__detail-bathroom'})]\n",
        "    garage_lists = [garage.text.lower() for garage in vr_soup.find_all('li', {'class':'property-card__detail-garage'})]\n",
        "    adress_lists = [adress.text.lower() for adress in vr_soup.find_all('span', {'class':'property-card__address'})]\n",
        "    price_lists = [price.text.lower() for price in vr_soup.find_all('div', {'class':'property-card__price'})]"
      ],
      "metadata": {
        "id": "GcwbG7-b0O5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Na parte final do c√≥digo, ainda dentro do while loop, iremos inserir o c√≥digo que far√° o valor da p√°gina trocar e iremos adicionar ao dataframe os valores/informa√ß√µes retiradas.\n",
        "\n",
        "Antes de iniciarmos o while loop, devemos criar a estrtura do data frame/tabela que queremos ter como resultado, definindo apenas as colunas."
      ],
      "metadata": {
        "id": "drlQp7-n5Uwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo a estrtura do data frame fora do while loop\n",
        "df = pd.DataFrame(columns=['product', \n",
        "                           'area', \n",
        "                           'room', \n",
        "                           'bath', \n",
        "                           'garage', \n",
        "                           'adress', \n",
        "                           'price'])"
      ],
      "metadata": {
        "id": "9K69tmnj5Y_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tendo este data frame criado, √© poss√≠vel inserir novas informa√ß√µes nele por meio do uso do 'append', em formato de DF contendo em um dicion√°rio coluna : lista de valores.\n",
        "\n",
        "Al√©m disso teremos como defini√ß√£o do n√∫mero da p√°gina, o valor da p√°gina daquele loop + 1. Quando a p√°gina for igual a 4, o while n√£o ir√° continuar, uma vez que o retorno ser√° False por n√£o ser menor ou igual a 3."
      ],
      "metadata": {
        "id": "6IiTvVd56mRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    page += 1\n",
        "    df = df.append(pd.DataFrame({'product':prod_lists, \n",
        "                                 'area':area_lists, \n",
        "                                 'room':room_lists, \n",
        "                                 'bath':bath_lists, \n",
        "                                 'garage':garage_lists, \n",
        "                                 'adress':adress_lists, \n",
        "                                 'price':price_lists}))"
      ],
      "metadata": {
        "id": "KwIxDjMy5VDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saindo do while loop, podemos verificar as informa√ß√µes do DF e export√°-lo para a m√°quina local"
      ],
      "metadata": {
        "id": "rrxd8bwf75gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head() #verificar as 5 primeiras linhas\n",
        "df.shape() #verificar n√∫mero de linhas e colunas \n",
        "df.duplicated() #verificar se h√° valores duplicados"
      ],
      "metadata": {
        "id": "TA_DqUt376W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('imoveis.csv') #exportar df para m√°quina local"
      ],
      "metadata": {
        "id": "BBAgJoqE761W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
